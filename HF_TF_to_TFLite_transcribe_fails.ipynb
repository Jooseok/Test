{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jooseok/Test/blob/main/HF_TF_to_TFLite_transcribe_fails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOv0nm8HX9Fr",
        "outputId": "c07e95b9-6530-4281-8837-51f49a152df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "YJhXm0dCt1w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "id": "gZtBiDQTxHn6",
        "outputId": "5efd2694-f273-4f15-b730-eaaa01888ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor, TFWhisperForConditionalGeneration, GenerationConfig\n",
        "from datasets import Audio, load_dataset\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import TFForceTokensLogitsProcessor, TFLogitsProcessor\n",
        "from typing import List, Optional, Union, Any\n",
        "\n",
        "# load model and processor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
        "model = TFWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"es\", task=\"translate\")\n",
        "#forced_decoder_ids = processor.get_decoder_prompt_ids(task=\"transcribe\")\n",
        "\n",
        "# load streaming dataset and read first audio sample\n",
        "ds = load_dataset(\"common_voice\", \"es\", split=\"test\", streaming=True)\n",
        "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "input_speech = next(iter(ds))[\"audio\"]\n",
        "input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"tf\").input_features\n",
        "model.config.forced_decoder_ids = forced_decoder_ids\n",
        "generation_config = GenerationConfig(force_token_map=forced_decoder_ids)\n",
        "print(generation_config)\n",
        "# generate token ids\n",
        "\n",
        "\n",
        "# Patching methods of class TFForceTokensLogitsProcessor(TFLogitsProcessor):\n",
        "\n",
        "def my__init__(self, force_token_map: List[List[int]]):\n",
        "    force_token_map = dict(force_token_map)\n",
        "    # Converts the dictionary of format {index: token} containing the tokens to be forced to an array, where the\n",
        "    # index of the array corresponds to the index of the token to be forced, for XLA compatibility.\n",
        "    # Indexes without forced tokens will have an negative value.\n",
        "    force_token_array = np.ones((max(force_token_map.keys()) + 1), dtype=np.int32) * -1\n",
        "    for index, token in force_token_map.items():\n",
        "        if token is not None:\n",
        "            force_token_array[index] = token\n",
        "    self.force_token_array = tf.convert_to_tensor(force_token_array, dtype=tf.int32)\n",
        "\n",
        "def my__call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n",
        "    def _force_token(generation_idx):\n",
        "        batch_size = scores.shape[0]\n",
        "        current_token = self.force_token_array[generation_idx]\n",
        "\n",
        "        # Original code below generates NaN values when the model is exported to tflite\n",
        "        # it just needs to be a negative number so that the forced token's value of 0 is the largest\n",
        "        # so it will get chosen\n",
        "        #new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(\"inf\")\n",
        "        new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(1)\n",
        "        indices = tf.stack((tf.range(batch_size), tf.tile([current_token], [batch_size])), axis=1)\n",
        "        updates = tf.zeros((batch_size,), dtype=scores.dtype)\n",
        "        new_scores = tf.tensor_scatter_nd_update(new_scores, indices, updates)\n",
        "        return new_scores\n",
        "\n",
        "    scores = tf.cond(\n",
        "        tf.greater_equal(cur_len, tf.shape(self.force_token_array)[0]),\n",
        "        # If the current length is geq than the length of force_token_array, the processor does nothing.\n",
        "        lambda: tf.identity(scores),\n",
        "        # Otherwise, it may force a certain token.\n",
        "        lambda: tf.cond(\n",
        "            tf.greater_equal(self.force_token_array[cur_len], 0),\n",
        "            # Only valid (positive) tokens are forced\n",
        "            lambda: _force_token(cur_len),\n",
        "            # Otherwise, the processor does nothing.\n",
        "            lambda: scores,\n",
        "        ),\n",
        "    )\n",
        "    return scores\n",
        "\n",
        "TFForceTokensLogitsProcessor.__init__ = my__init__\n",
        "TFForceTokensLogitsProcessor.__call__ = my__call__\n",
        "predicted_ids = model.generate(input_features,generation_config=generation_config)\n",
        "print(predicted_ids)\n",
        "# decode token ids to text\n",
        "transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVrZjl_GYL93",
        "outputId": "28bfd38f-875d-49b3-f72e-f05d4ede6e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFWhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of TFWhisperForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWhisperForConditionalGeneration for predictions without further training.\n",
            "/root/.cache/huggingface/modules/datasets_modules/datasets/common_voice/220833898d6a60c50f621126e51fb22eb2dfe5244392c70dccd8e6e2f055f4bf/common_voice.py:634: FutureWarning: \n",
            "            This version of the Common Voice dataset is deprecated.\n",
            "            You can download the latest one with\n",
            "            >>> load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\")\n",
            "            \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"force_token_map\": [\n",
            "    [\n",
            "      1,\n",
            "      50262\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50358\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:838: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[50258 50262 50358 50363   731   293   286   486   747   309   484   700\n",
            "  50257]], shape=(1, 13), dtype=int32)\n",
            "[' well and I will take it out first']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/tf_whisper_saved')"
      ],
      "metadata": {
        "id": "c2QDpzWmZD61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class GenerateModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    super(GenerateModel, self).__init__()\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "    input_signature=[\n",
        "      tf.TensorSpec(shape=(1, 80,3000), dtype=tf.float32, name=\"input_ids\"),\n",
        "    ]\n",
        "  )\n",
        "  def serving(self, input_ids):\n",
        "    outputs = self.model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
        "    return {\"sequences\": outputs}\n",
        "\n",
        "# Saving the model\n",
        "saved_model_dir = '/content/tf_whisper_saved'\n",
        "generate_model = GenerateModel(model=model)\n",
        "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})\n",
        "\n",
        "# Converting to TFLite model\n",
        "tflite_model_path = '/content/whisper-tiny.tflite'\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Saving the TFLite model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "i7HVhtrnYwqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpBqxZlLaY_5",
        "outputId": "71874ad1-c44c-43f4-d07e-5f42a9f8ae0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 150260\n",
            "drwxr-xr-x 1 root root      4096 Oct 24 03:07 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root root      4096 Oct 24 03:02 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 4 root root      4096 Oct 20 13:20 \u001b[01;34m.config\u001b[0m/\n",
            "drwxr-xr-x 1 root root      4096 Oct 20 13:20 \u001b[01;34msample_data\u001b[0m/\n",
            "drwxr-xr-x 4 root root      4096 Oct 24 03:33 \u001b[01;34mtf_whisper_saved\u001b[0m/\n",
            "-rw-r--r-- 1 root root 153845620 Oct 24 03:34 whisper-tiny.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywSQJN5ndt2q",
        "outputId": "0d9945ec-2d1d-4e40-f754-591510671e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: whisper in /usr/local/lib/python3.10/dist-packages (1.1.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from whisper) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import whisper\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "# Define the path to the TFLite model\n",
        "tflite_model_path = '/content/whisper-tiny.tflite'\n",
        "\n",
        "# Create an interpreter to run the TFLite model\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "\n",
        "# Allocate memory for the interpreter\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get the input and output tensors\n",
        "input_tensor = interpreter.get_input_details()[0]['index']\n",
        "output_tensor = interpreter.get_output_details()[0]['index']\n",
        "\n",
        "\n",
        "inference_start = timer()\n",
        "\n",
        "# Run the TFLite model using the interpreter\n",
        "print(\"Invoking interpreter ...\")\n",
        "interpreter.set_tensor(input_tensor, input_features)\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output data from the interpreter\n",
        "output_data = interpreter.get_tensor(output_tensor)\n",
        "\n",
        "# Print the output data\n",
        "print(output_data)\n",
        "transcription = processor.batch_decode(output_data, skip_special_tokens=True)\n",
        "print(transcription)\n",
        "# Create a tokenizer to convert tokens to text\n",
        "#wtokenizer = whisper.tokenizer.get_tokenizer(True, language=\"fr\")\n",
        "\n",
        "# convert tokens to text\n",
        "#print(\"Converting tokens ...\")\n",
        "#for token in output_data:\n",
        "    # Replace -100 with the end of text token\n",
        "  #  token[token == -100] = wtokenizer.eot\n",
        "    #text = wtokenizer.decode(token, skip_special_tokens=True)\n",
        "  #  print(text)\n",
        "\n",
        "print(\"\\nInference took {:.2f}s \".format(timer() - inference_start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcMPskKqabLy",
        "outputId": "a0ea49b7-27fa-4dd5-8946-f7c2d4848dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invoking interpreter ...\n",
            "[[50258 50262 50358 50363   731   293   286   486   747   309   484   700\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257\n",
            "  50257 50257 50257 50257]]\n",
            "[' well and I will take it out first']\n",
            "\n",
            "Inference took 5.75s \n"
          ]
        }
      ]
    }
  ]
}